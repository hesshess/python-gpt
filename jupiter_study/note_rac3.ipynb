{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1\n",
    ")\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/document.txt\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# list of docs\n",
    "\n",
    "# for doc in list of docs | prompt | llm\n",
    "\n",
    "# for response in list of llms response | put them all together \n",
    "\n",
    "# final doc | prompt | llm \n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"\"\"\n",
    "     Use the following portion of a ling document to see if any of the text is relevant to answer the question. Return any relevant text verbatim\n",
    "     -------\n",
    "     {portion}\n",
    "     \"\"\"),\n",
    "    ('human', \"{question}\")\n",
    "])\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs['documents']\n",
    "    question = inputs['question']\n",
    "    # results = [] \n",
    "    # for document in documents:\n",
    "    #     result = map_doc_chain.invoke({'portion': document.page_content, 'question': question }).content\n",
    "    #     results.append(result )\n",
    "    #     results = \"\\n\\n\".join(results)\n",
    "    return \"\\n\\n\".join(map_doc_chain.invoke({\n",
    "        'portion': doc.page_content,\n",
    "        'question' : question  }).content for doc in documents \n",
    "        )\n",
    "\n",
    "map_chain = {'documents': retriever, 'question': RunnablePassthrough()} | RunnableLambda(map_docs)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"\"\"\n",
    "     Given the following extracted parts of  a long document and a question, create a final answer.\n",
    "     If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "     ------\n",
    "     {context}\n",
    "     \"\"\"),\n",
    "     ('human', '{question}'),\n",
    "])\n",
    "chain = {'context' : map_chain ,'question': RunnablePassthrough()} | final_prompt | llm\n",
    "\n",
    "chain.invoke(\"Describe Victory Mansions\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
